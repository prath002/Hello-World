{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwF1xZxf6VKs"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "from tensorflow.python.keras import utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=open('CBOW.txt','r')\n",
        "c_data = [text for text in data if text.count(' ') >= 2]"
      ],
      "metadata": {
        "id": "Z1d7sm_-6arx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize = Tokenizer()\n",
        "vectorize.fit_on_texts(c_data)"
      ],
      "metadata": {
        "id": "cs8ml0p_8RH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of words in all sentences combined\n",
        "total_vocab = sum(len(s) for s in c_data)\n",
        "\n",
        "# Calculate the total number of unique words in the vocabulary\n",
        "word_count = len(vectorize.word_index) + 1\n",
        "\n",
        "print(total_vocab)\n",
        "print(word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnjwkXXQ-VQq",
        "outputId": "8cd8cb7c-bd6d-49f8-9da3-9e179d0b8f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1191\n",
            "103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 2\n",
        "total_length = window_size * 2\n",
        "embedding_dim = 100\n",
        "\n",
        "# Function to generate CBOW data\n",
        "def generate_cbow_data(data, window_size):\n",
        "    for text in data:\n",
        "        text_len = len(text)\n",
        "        for idx, word in enumerate(text):\n",
        "            context_word = [text[i] for i in range(idx - window_size, idx + window_size + 1) if 0 <= i < text_len and i != idx]\n",
        "            target = [word]\n",
        "            contextual = sequence.pad_sequences([context_word], maxlen=total_length)\n",
        "            final_target = to_categorical(target, total_vocab)\n",
        "            yield contextual, final_target\n",
        "\n",
        "# CBOW model\n",
        "cbow_model = Sequential()\n",
        "cbow_model.add(Embedding(input_dim=total_vocab, output_dim=embedding_dim))\n",
        "cbow_model.add(Lambda(lambda x: K.mean(x, axis=1)))\n",
        "cbow_model.add(Dense(total_vocab, activation='softmax'))\n",
        "cbow_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Training the CBOW model\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    cost = 0\n",
        "    for x, y in generate_cbow_data(data, window_size):\n",
        "        cost += cbow_model.train_on_batch(x, y)\n",
        "    print(f'Epoch {epoch + 1}, Loss: {cost}')\n",
        "\n",
        "# Save word vectors\n",
        "word_vectors = cbow_model.get_weights()[0]\n",
        "\n",
        "with open('vectors.txt', 'w') as vect_file:\n",
        "    vect_file.write(f'{total_vocab} {embedding_dim}\\n')\n",
        "    for word, i in vectorize.word_index.items():\n",
        "        vect_file.write(f'{word} {\" \".join(map(str, word_vectors[i, :]))}\\n')\n",
        "\n",
        "import gensim\n",
        "cbow_output = gensim.models.KeyedVectors.load_word2vec_format('vectors.txt', binary=False, limit=100)\n",
        "\n",
        "# Get similar words\n",
        "cbow_output.most_similar(positive=['covid'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC8kwNIWJH6N",
        "outputId": "723216dc-1343-4791-a047-23688bfda691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0\n",
            "Epoch 2, Loss: 0\n",
            "Epoch 3, Loss: 0\n",
            "Epoch 4, Loss: 0\n",
            "Epoch 5, Loss: 0\n",
            "Epoch 6, Loss: 0\n",
            "Epoch 7, Loss: 0\n",
            "Epoch 8, Loss: 0\n",
            "Epoch 9, Loss: 0\n",
            "Epoch 10, Loss: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('who', 0.2629571557044983),\n",
              " ('48', 0.24375006556510925),\n",
              " ('this', 0.23177707195281982),\n",
              " ('infections', 0.205921933054924),\n",
              " ('is', 0.19043906033039093),\n",
              " ('does', 0.16252131760120392),\n",
              " ('illness', 0.16196690499782562),\n",
              " ('the', 0.14322620630264282),\n",
              " ('and', 0.14139805734157562),\n",
              " ('19', 0.14037524163722992)]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VRkVVfOCTULF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}